{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Importing libraries**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.naive_bayes import MultinomialNB\n",
            "from sklearn.svm import LinearSVC\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "from sklearn.utils import shuffle\n",
            "from scipy.sparse import csr_matrix\n",
            "import tensorflow as tf\n",
            "import re\n",
            "import seaborn as sns\n",
            "from wordcloud import WordCloud\n",
            "import matplotlib.pyplot as plt\n",
            "from joblib import dump,load\n",
            "from sklearn.metrics import precision_recall_fscore_support as score\n",
            "import numpy as np\n",
            "from imblearn.under_sampling import RandomUnderSampler"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Read Dataset/Calculating weight**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "df = pd.read_json(\"./Video_Games_5.json\", lines=True)\n",
            "num_overall_1 = df[\"overall\"].value_counts()[1]\n",
            "num_overall_2 = df[\"overall\"].value_counts()[2]\n",
            "num_overall_3 = df[\"overall\"].value_counts()[3]\n",
            "num_overall_4 = df[\"overall\"].value_counts()[4]\n",
            "num_overall_5 = df[\"overall\"].value_counts()[5]\n",
            "num_class_0 = num_overall_1 + num_overall_2\n",
            "num_class_2 = num_overall_4 + num_overall_5\n",
            "weight_0 = num_class_2 / num_class_0\n",
            "weight_1 = num_class_2 / num_overall_3\n",
            "bayes_weight=[0.4,0.4,0.2]\n",
            "svm_rf_weight={0:weight_0, 1: weight_1, 2:4}"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Creating classes**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "df[\"sentiment\"] = df[\"overall\"].apply(\n",
            "    lambda rating: 0 if rating <= 2 else (1 if rating == 3 else 2)\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Preprocessing**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "df.fillna({\"reviewText\": \"\"}, inplace=True)\n",
            "\n",
            "df[\"reviewText\"] = df[\"reviewText\"].apply(lambda x: re.sub(r\"\\W\", \" \", str(x)))\n",
            "\n",
            "stop_words = set(stopwords.words(\"english\"))\n",
            "df[\"reviewText\"] = df[\"reviewText\"].apply(\n",
            "    lambda x: \" \".join([word for word in word_tokenize(x) if word not in stop_words])\n",
            ")\n",
            "\n",
            "stemmer = PorterStemmer()\n",
            "df[\"reviewText\"] = df[\"reviewText\"].apply(\n",
            "    lambda x: \" \".join([stemmer.stem(word) for word in word_tokenize(x)])\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Vectorization**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectorizer = TfidfVectorizer()\n",
            "reviews_tfidf = vectorizer.fit_transform(df[\"reviewText\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#UnderSampling**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "rus=RandomUnderSampler(random_state=42, sampling_strategy={2: 50000})\n",
            "reviews_sampled, sentiments_sampled = rus.fit_resample(reviews_tfidf, df[\"sentiment\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Splitting dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "reviews_train, reviews_test, sentiments_train, sentiments_test = train_test_split(\n",
            "    reviews_sampled, sentiments_sampled, test_size=0.2, random_state=42\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Random Forest Classifier**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = RandomForestClassifier(n_estimators=300, max_depth=100, min_samples_leaf=5, min_samples_split=10, random_state=42, class_weight=svm_rf_weight)\n",
            "\n",
            "clf.fit(reviews_train, sentiments_train)\n",
            "\n",
            "dump(clf, 'random_forest_classifier_partition.joblib')\n",
            "rf_loaded = load('random_forest_classifier_partition.joblib')\n",
            "\n",
            "sentiments_pred = rf_loaded.predict(reviews_test)\n",
            "train_accuracy = accuracy_score(sentiments_train, rf_loaded.predict(reviews_train))\n",
            "test_accuracy = accuracy_score(sentiments_test, rf_loaded.predict(reviews_test))\n",
            "print(\"Train accuracy: \", train_accuracy)\n",
            "print(\"Test accuracy: \", test_accuracy)\n",
            "print(classification_report(sentiments_test, sentiments_pred))\n",
            "\n",
            "cm = confusion_matrix(sentiments_test, sentiments_pred)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#SVM Classifier**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = LinearSVC(dual=True, max_iter=10000, class_weight=svm_rf_weight)\n",
            "\n",
            "clf.fit(reviews_train, sentiments_train)\n",
            "\n",
            "dump(clf, 'svm_partition.joblib')\n",
            "svm_loaded = load('svm.joblib')\n",
            "\n",
            "sentiments_pred = clf.predict(reviews_test)\n",
            "train_accuracy = accuracy_score(sentiments_train, clf.predict(reviews_train))\n",
            "test_accuracy = accuracy_score(sentiments_test, clf.predict(reviews_test))\n",
            "print(\"Train accuracy: \", train_accuracy)\n",
            "print(\"Test accuracy: \", test_accuracy)\n",
            "print(classification_report(sentiments_test, sentiments_pred))\n",
            "\n",
            "cm = confusion_matrix(sentiments_test, sentiments_pred)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Multinomial Naive Bayes Classifier**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = MultinomialNB(class_prior=bayes_weight)\n",
            "clf.fit(reviews_train, sentiments_train)\n",
            "dump(clf, \"bayes_partition.joblib\")\n",
            "\n",
            "nb_loaded = load('bayes.joblib')\n",
            "sentiments_pred = clf.predict(reviews_test)\n",
            "\n",
            "print(classification_report(sentiments_test, sentiments_pred))\n",
            "\n",
            "train_accuracy = accuracy_score(sentiments_train, clf.predict(reviews_train))\n",
            "test_accuracy = accuracy_score(sentiments_test, clf.predict(reviews_test))\n",
            "print(\"Train accuracy: \", train_accuracy)\n",
            "print(\"Test accuracy: \", test_accuracy)\n",
            "\n",
            "cm = confusion_matrix(sentiments_test, sentiments_pred)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#WordCloud**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "positive_reviews = df[df['sentiment'] == 2]['reviewText'].str.cat(sep=' ')\n",
            "wordcloud = WordCloud(background_color='white', max_words=200).generate(positive_reviews)\n",
            "plt.figure(figsize=(10, 10))\n",
            "plt.imshow(wordcloud, interpolation='bilinear')\n",
            "plt.axis('off')\n",
            "plt.title('Word Cloud for Positive Reviews')\n",
            "plt.show()\n",
            "\n",
            "negative_reviews = df[df['sentiment'] == 0]['reviewText'].str.cat(sep=' ')\n",
            "wordcloud_negative = WordCloud(background_color='white', max_words=200, contour_color='red').generate(negative_reviews)\n",
            "plt.figure(figsize=(10, 10))\n",
            "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
            "plt.axis('off')\n",
            "plt.title('Negative Sentiment Word Cloud')\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Metrics Graph**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "svm_loaded = load('svm_partition.joblib')\n",
            "nb_loaded = load('bayes_partition.joblib')\n",
            "precision_rf, recall_rf, fscore_rf, _ = score(sentiments_test, rf_loaded.predict(reviews_test), average='macro')\n",
            "\n",
            "precision_svm, recall_svm, fscore_svm, _ = score(sentiments_test, svm_loaded.predict(reviews_test), average='macro')\n",
            "\n",
            "precision_nb, recall_nb, fscore_nb, _ = score(sentiments_test, nb_loaded.predict(reviews_test), average='macro')\n",
            "\n",
            "models = ['Random Forest', 'SVM', 'Naive Bayes']\n",
            "precision_scores = [precision_rf, precision_svm, precision_nb]\n",
            "recall_scores = [recall_rf, recall_svm, recall_nb]\n",
            "fscore_scores = [fscore_rf, fscore_svm, fscore_nb]\n",
            "\n",
            "x = np.arange(len(models))  \n",
            "width = 0.25  \n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision_scores, width, label='Precision')\n",
            "rects2 = ax.bar(x, recall_scores, width, label='Recall')\n",
            "rects3 = ax.bar(x + width, fscore_scores, width, label='F1-Score')\n",
            "\n",
            "ax.set_ylabel('Scores')\n",
            "ax.set_title('Scores by model and metric')\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(models)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Rete Neurale Fully Connected**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "input_size = reviews_tfidf.shape[1]\n",
            "\n",
            "input_layer = tf.keras.Input(shape=(input_size,))\n",
            "hidden_layer = tf.keras.layers.Dense(72, activation='relu')(input_layer)\n",
            "output_layer = tf.keras.layers.Dense(3, activation='softmax')(hidden_layer)\n",
            "\n",
            "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
            "\n",
            "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model.fit(reviews_train, sentiments_train, epochs=50, batch_size=256, shuffle=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "predictions = model.predict(reviews_test)\n",
            "\n",
            "predicted_classes = predictions.argmax(axis=1) \n",
            "\n",
            "real_labels = sentiments_test.astype(int)\n",
            "\n",
            "\n",
            "print(classification_report(real_labels, predicted_classes))\n",
            "print(\"\\nAccuracy:\", accuracy_score(real_labels, predicted_classes))\n",
            "\n",
            "cm = confusion_matrix(real_labels, predicted_classes)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")    \n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()\n",
            "\n",
            "report = classification_report(real_labels, predicted_classes, output_dict=True)\n",
            "precision = report[\"macro avg\"][\"precision\"]\n",
            "recall = report[\"macro avg\"][\"recall\"]\n",
            "f1_score = report[\"macro avg\"][\"f1-score\"]\n",
            "\n",
            "classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
            "\n",
            "x = np.arange(len(classes))\n",
            "width = 0.2\n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision, width, label=\"Precision\")\n",
            "rects2 = ax.bar(x, recall, width, label=\"Recall\")\n",
            "rects3 = ax.bar(x + width, f1_score, width, label=\"F1-score\")\n",
            "\n",
            "ax.set_ylabel(\"Score\")\n",
            "ax.set_title(\"Scores by  metric\")\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(classes)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**FCN With Dropout Layer**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "reviews_train_csr = csr_matrix(reviews_train)\n",
            "reviews_test_csr = csr_matrix(reviews_test)\n",
            "\n",
            "reviews_train_csr, sentiments_train_shuffle = shuffle(reviews_train_csr, sentiments_train)\n",
            "\n",
            "model = tf.keras.Sequential([\n",
            "    tf.keras.layers.Dense(72, activation='relu', input_shape=(reviews_train_csr.shape[1],)),\n",
            "    tf.keras.layers.Dropout(0.5),\n",
            "    tf.keras.layers.Dense(32, activation='relu'),\n",
            "    tf.keras.layers.Dropout(0.5),\n",
            "    tf.keras.layers.Dense(3, activation='softmax')\n",
            "])\n",
            "\n",
            "model.compile(optimizer='adam',\n",
            "              loss='sparse_categorical_crossentropy',\n",
            "              metrics=['accuracy'])\n",
            "\n",
            "model.fit(\n",
            "    reviews_train_csr, sentiments_train_shuffle, epochs=20, batch_size=256, shuffle=True\n",
            ")\n",
            "\n",
            "loss, accuracy = model.evaluate(reviews_test_csr, sentiments_test)\n",
            "print(\"Test Accuracy:\", accuracy)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "predictions = model.predict(reviews_test)\n",
            "\n",
            "\n",
            "predicted_classes = predictions.argmax(axis=1) \n",
            "\n",
            "real_labels = sentiments_test.astype(int)\n",
            "\n",
            "\n",
            "print(classification_report(real_labels, predicted_classes))\n",
            "print(\"\\nAccuracy:\", accuracy_score(real_labels, predicted_classes))\n",
            "\n",
            "cm = confusion_matrix(real_labels, predicted_classes)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")    \n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()\n",
            "\n",
            "report = classification_report(real_labels, predicted_classes, output_dict=True)\n",
            "precision = report[\"macro avg\"][\"precision\"]\n",
            "recall = report[\"macro avg\"][\"recall\"]\n",
            "f1_score = report[\"macro avg\"][\"f1-score\"]\n",
            "\n",
            "classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
            "\n",
            "x = np.arange(len(classes))\n",
            "width = 0.2\n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision, width, label=\"Precision\")\n",
            "rects2 = ax.bar(x, recall, width, label=\"Recall\")\n",
            "rects3 = ax.bar(x + width, f1_score, width, label=\"F1-score\")\n",
            "\n",
            "ax.set_ylabel(\"Score\")\n",
            "ax.set_title(\"Scores by  metric\")\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(classes)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#1DCNN**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# BEFORE STARTING CNN CODE\n",
            "# DOWNLOAD glove.6B.200d.txt FROM THE FOLLOWING LINK AND PLACE IT IN THE SAME DIRECTORY AS THIS SCRIPT\n",
            "# https://www.kaggle.com/datasets/rtatman/glove-global-vectors-for-word-representation?resource=download"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "def load_glove_vectors(file_path):\n",
            "    word_vectors = {}\n",
            "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
            "        for line in file:\n",
            "            parts = line.split()\n",
            "            word = parts[0]\n",
            "            vector = np.array([float(val) for val in parts[1:]])\n",
            "            word_vectors[word] = vector\n",
            "    return word_vectors\n",
            "\n",
            "glove_file_path = \"./glove.6B.200d.txt\"\n",
            "glove_vectors = load_glove_vectors(glove_file_path)\n",
            "\n",
            "vocab_size = len(glove_vectors)\n",
            "embedding_dim = len(next(iter(glove_vectors.values())))\n",
            "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
            "word_index = {}\n",
            "for i, (word, vector) in enumerate(glove_vectors.items()):\n",
            "    embedding_matrix[i] = vector\n",
            "    word_index[word] = i"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "modelCNN = tf.keras.Sequential(\n",
            "    [\n",
            "        tf.keras.layers.Embedding(\n",
            "            input_dim=vocab_size,\n",
            "            output_dim=embedding_dim,\n",
            "            weights=[embedding_matrix],\n",
            "            trainable=False,\n",
            "        ),\n",
            "        tf.keras.layers.Conv1D(64, 5, activation=\"relu\"),\n",
            "        tf.keras.layers.MaxPooling1D(pool_size=4),\n",
            "        tf.keras.layers.Flatten(),\n",
            "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
            "        tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
            "    ]\n",
            ")\n",
            "\n",
            "modelCNN.compile(\n",
            "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
            ")\n",
            "\n",
            "\n",
            "modelCNN.fit(\n",
            "    reviews_train,\n",
            "    sentiments_train,\n",
            "    epochs=5,\n",
            "    batch_size=32,\n",
            "    validation_split=0.1,\n",
            ")\n",
            "\n",
            "loss, accuracy = modelCNN.evaluate(reviews_test, sentiments_test)\n",
            "\n",
            "print(\"Test Accuracy:\", accuracy)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "predictions = modelCNN.predict(reviews_test)\n",
            "\n",
            "\n",
            "predicted_classes = predictions.argmax(\n",
            "    axis=1\n",
            ")\n",
            "\n",
            "real_labels = sentiments_test.astype(int)\n",
            "\n",
            "\n",
            "print(classification_report(real_labels, predicted_classes))\n",
            "print(\"\\nAccuracy:\", accuracy_score(real_labels, predicted_classes))\n",
            "\n",
            "cm = confusion_matrix(real_labels, predicted_classes)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()\n",
            "\n",
            "report = classification_report(real_labels, predicted_classes, output_dict=True)\n",
            "precision = report[\"macro avg\"][\"precision\"]\n",
            "recall = report[\"macro avg\"][\"recall\"]\n",
            "f1_score = report[\"macro avg\"][\"f1-score\"]\n",
            "\n",
            "classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
            "\n",
            "x = np.arange(len(classes))\n",
            "width = 0.2\n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision, width, label=\"Precision\")\n",
            "rects2 = ax.bar(x, recall, width, label=\"Recall\")\n",
            "rects3 = ax.bar(x + width, f1_score, width, label=\"F1-score\")\n",
            "\n",
            "ax.set_ylabel(\"Score\")\n",
            "ax.set_title(\"Scores by  metric\")\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(classes)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#HAN**\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "vocab = vectorizer.get_feature_names_out()\n",
            "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
            "\n",
            "train_sequence = []\n",
            "for doc in reviews_train:\n",
            "    doc = doc.toarray().flatten()\n",
            "    sequence = [word_to_index[vocab[i]] for i in np.where(doc > 0)[0]]\n",
            "    train_sequence.append(sequence)\n",
            "\n",
            "max_sequence_length = max(len(seq) for seq in train_sequence)\n",
            "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
            "    train_sequence, maxlen=max_sequence_length, padding=\"post\"\n",
            ")\n",
            "\n",
            "test_sequences = []\n",
            "for doc in reviews_test:\n",
            "    doc = doc.toarray().flatten()\n",
            "    words = [word for word, count in zip(vocab, doc) if count > 0]\n",
            "    words_lower = [word.lower() for word in words]\n",
            "    doc_lower = \" \".join(words_lower)\n",
            "    doc_vector = vectorizer.transform([doc_lower])\n",
            "    sequence = [\n",
            "        word_to_index[word] for word, count in zip(vocab, doc_vector.toarray().flatten()) if count > 0\n",
            "    ]\n",
            "    test_sequences.append(sequence)\n",
            "\n",
            "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
            "    test_sequences, maxlen=max_sequence_length, padding=\"post\"\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def attention_layer(inputs):\n",
            "    attention = tf.keras.layers.Dense(1, activation=\"tanh\")(inputs)\n",
            "    attention = tf.keras.layers.Flatten()(attention)\n",
            "    attention = tf.keras.layers.Activation(\"softmax\")(attention)\n",
            "    attention = tf.keras.layers.RepeatVector(2 * 64)(attention)\n",
            "    attention = tf.keras.layers.Permute([2, 1])(attention)\n",
            "    return tf.keras.layers.multiply([inputs, attention])\n",
            "\n",
            "def create_han(max_sequence_length, max_words, embedding_dim):\n",
            "    input_word = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
            "    embedding = tf.keras.layers.Embedding(\n",
            "        input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length\n",
            "    )(input_word)\n",
            "    word_encoder = tf.keras.layers.Bidirectional(\n",
            "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
            "    )(embedding)\n",
            "    word_attention = attention_layer(word_encoder)\n",
            "    word_attention = tf.keras.layers.Lambda(\n",
            "        lambda x: tf.keras.backend.sum(x, axis=1), output_shape=(128,)\n",
            "    )(word_attention)\n",
            "\n",
            "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(word_attention)\n",
            "\n",
            "    model = tf.keras.models.Model(inputs=input_word, outputs=output)\n",
            "    return model\n",
            "\n",
            "max_sequence_length = train_sequences_padded.shape[\n",
            "    1\n",
            "]\n",
            "max_words = len(word_to_index) + 1 \n",
            "embedding_dim = 100\n",
            "\n",
            "modelHAN = create_han(max_sequence_length, max_words, embedding_dim)\n",
            "\n",
            "modelHAN.compile(\n",
            "    loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
            ")\n",
            "sentiments_train_onehot = tf.keras.utils.to_categorical(sentiments_train, num_classes=3)\n",
            "sentiments_test_onehot = tf.keras.utils.to_categorical(sentiments_test, num_classes=3)\n",
            "\n",
            "modelHAN.fit(\n",
            "    train_sequences_padded,\n",
            "    sentiments_train_onehot,\n",
            "    epochs=10,\n",
            "    batch_size=64,\n",
            "    validation_data=(test_sequences_padded, sentiments_test_onehot),\n",
            ")\n",
            "\n",
            "modelHAN.summary()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "predictions = modelHAN.predict(test_sequences_padded)\n",
            "\n",
            "predicted_classes = predictions.argmax(\n",
            "    axis=1\n",
            ")\n",
            "\n",
            "real_labels = sentiments_test.astype(int)\n",
            "\n",
            "\n",
            "print(classification_report(real_labels, predicted_classes))\n",
            "print(\"\\nAccuracy:\", accuracy_score(real_labels, predicted_classes))\n",
            "\n",
            "cm = confusion_matrix(real_labels, predicted_classes)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()\n",
            "\n",
            "report = classification_report(real_labels, predicted_classes, output_dict=True)\n",
            "precision = report[\"macro avg\"][\"precision\"]\n",
            "recall = report[\"macro avg\"][\"recall\"]\n",
            "f1_score = report[\"macro avg\"][\"f1-score\"]\n",
            "\n",
            "classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
            "\n",
            "x = np.arange(len(classes))\n",
            "width = 0.2\n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision, width, label=\"Precision\")\n",
            "rects2 = ax.bar(x, recall, width, label=\"Recall\")\n",
            "rects3 = ax.bar(x + width, f1_score, width, label=\"F1-score\")\n",
            "ax.set_ylabel(\"Score\")\n",
            "ax.set_title(\"Scores by  metric\")\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(classes)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#DISTIL BERT**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            "\n",
            "train_texts = [\n",
            "    \" \".join(\n",
            "        [\n",
            "            vocab[index]\n",
            "            for index, count in enumerate(doc.toarray().flatten())\n",
            "            if count > 0\n",
            "        ]\n",
            "    )\n",
            "    for doc in reviews_train\n",
            "]\n",
            "test_texts = [\n",
            "    \" \".join(\n",
            "        [\n",
            "            vocab[index]\n",
            "            for index, count in enumerate(doc.toarray().flatten())\n",
            "            if count > 0\n",
            "        ]\n",
            "    )\n",
            "    for doc in reviews_test\n",
            "]\n",
            "\n",
            "train_tokenizer = [\n",
            "    tokenizer.encode(review, max_length=512, truncation=True) for review in train_texts\n",
            "]\n",
            "test_tokenizer = [\n",
            "    tokenizer.encode(review, max_length=512, truncation=True) for review in test_texts\n",
            "]\n",
            "\n",
            "max_seq_length= max(len(seq) for seq in train_tokenizer + test_tokenizer)\n",
            "train_tokenizer_padded = tf.keras.preprocessing.sequence.pad_sequences(train_tokenizer, maxlen=max_seq_length, padding='post')\n",
            "test_tokenizer_padded = tf.keras.preprocessing.sequence.pad_sequences(test_tokenizer, maxlen=max_seq_length, padding='post')\n",
            "\n",
            "train_tokenizer_padded = tf.convert_to_tensor(train_tokenizer_padded)\n",
            "test_tokenizer_padded = tf.convert_to_tensor(test_tokenizer_padded)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "modelDB = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
            "\n",
            "modelDB.compile(\n",
            "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
            ")\n",
            "\n",
            "modelDB.fit(\n",
            "    train_tokenizer_padded,\n",
            "    sentiments_train,\n",
            "    epochs=1,\n",
            "    batch_size=32,\n",
            "    validation_split=0.1,\n",
            ")\n",
            "\n",
            "modelDB.summary()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "predictions = modelDB.predict(test_tokenizer_padded)\n",
            "\n",
            "predicted_classes = predictions.argmax(\n",
            "    axis=1\n",
            ") \n",
            "real_labels = sentiments_test.astype(int)\n",
            "\n",
            "\n",
            "print(classification_report(real_labels, predicted_classes))\n",
            "print(\"\\nAccuracy:\", accuracy_score(real_labels, predicted_classes))\n",
            "\n",
            "cm = confusion_matrix(real_labels, predicted_classes)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()\n",
            "\n",
            "report = classification_report(real_labels, predicted_classes, output_dict=True)\n",
            "precision = report[\"macro avg\"][\"precision\"]\n",
            "recall = report[\"macro avg\"][\"recall\"]\n",
            "f1_score = report[\"macro avg\"][\"f1-score\"]\n",
            "\n",
            "classes = [\"Negative\", \"Neutral\", \"Positive\"]\n",
            "\n",
            "x = np.arange(len(classes))\n",
            "width = 0.2\n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision, width, label=\"Precision\")\n",
            "rects2 = ax.bar(x, recall, width, label=\"Recall\")\n",
            "rects3 = ax.bar(x + width, f1_score, width, label=\"F1-score\")\n",
            "ax.set_ylabel(\"Score\")\n",
            "ax.set_title(\"Scores by  metric\")\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(classes)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.2"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
