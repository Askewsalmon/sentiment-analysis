{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Importing libraries**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.naive_bayes import MultinomialNB\n",
            "from sklearn.svm import LinearSVC\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.stem import PorterStemmer\n",
            "from nltk.tokenize import word_tokenize\n",
            "import re\n",
            "import seaborn as sns\n",
            "from wordcloud import WordCloud\n",
            "import matplotlib.pyplot as plt\n",
            "from joblib import dump,load\n",
            "from sklearn.metrics import precision_recall_fscore_support as score\n",
            "import numpy as np\n",
            "from imblearn.under_sampling import RandomUnderSampler"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Read Dataset/Calculating weight**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "df = pd.read_json(\"./Video_Games_5.json\", lines=True)\n",
            "num_overall_1 = df[\"overall\"].value_counts()[1]\n",
            "num_overall_2 = df[\"overall\"].value_counts()[2]\n",
            "num_overall_3 = df[\"overall\"].value_counts()[3]\n",
            "num_overall_4 = df[\"overall\"].value_counts()[4]\n",
            "num_overall_5 = df[\"overall\"].value_counts()[5]\n",
            "num_class_0 = num_overall_1 + num_overall_2\n",
            "num_class_2 = num_overall_4 + num_overall_5\n",
            "weight_0 = num_class_2 / num_class_0\n",
            "weight_1 = num_class_2 / num_overall_3\n",
            "bayes_weight=[0.4,0.4,0.2]\n",
            "svm_rf_weight={0:weight_0, 1: weight_1, 2:4}"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Creating classes**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [],
         "source": [
            "df[\"sentiment\"] = df[\"overall\"].apply(\n",
            "    lambda rating: 0 if rating <= 2 else (1 if rating == 3 else 2)\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Preprocessing**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "df.fillna({\"reviewText\": \"\"}, inplace=True)\n",
            "\n",
            "df[\"reviewText\"] = df[\"reviewText\"].apply(lambda x: re.sub(r\"\\W\", \" \", str(x)))\n",
            "\n",
            "stop_words = set(stopwords.words(\"english\"))\n",
            "df[\"reviewText\"] = df[\"reviewText\"].apply(\n",
            "    lambda x: \" \".join([word for word in word_tokenize(x) if word not in stop_words])\n",
            ")\n",
            "\n",
            "stemmer = PorterStemmer()\n",
            "df[\"reviewText\"] = df[\"reviewText\"].apply(\n",
            "    lambda x: \" \".join([stemmer.stem(word) for word in word_tokenize(x)])\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Vectorization**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectorizer = TfidfVectorizer()\n",
            "reviews_tfidf = vectorizer.fit_transform(df[\"reviewText\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#UnderSampling**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [],
         "source": [
            "rus=RandomUnderSampler(random_state=42, sampling_strategy={2: 50000})\n",
            "reviews_sampled, sentiments_sampled = rus.fit_resample(reviews_tfidf, df[\"sentiment\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Splitting dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [],
         "source": [
            "reviews_train, reviews_test, sentiments_train, sentiments_test = train_test_split(\n",
            "    reviews_sampled, sentiments_sampled, test_size=0.2, random_state=42\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Random Forest Classifier**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = RandomForestClassifier(n_estimators=300, max_depth=100, min_samples_leaf=5, min_samples_split=10, random_state=42, class_weight=svm_rf_weight)\n",
            "\n",
            "clf.fit(reviews_train, sentiments_train)\n",
            "\n",
            "dump(clf, 'random_forest_classifier_partition.joblib')\n",
            "rf_loaded = load('random_forest_classifier_partition.joblib')\n",
            "\n",
            "sentiments_pred = rf_loaded.predict(reviews_test)\n",
            "train_accuracy = accuracy_score(sentiments_train, rf_loaded.predict(reviews_train))\n",
            "test_accuracy = accuracy_score(sentiments_test, rf_loaded.predict(reviews_test))\n",
            "print(\"Train accuracy: \", train_accuracy)\n",
            "print(\"Test accuracy: \", test_accuracy)\n",
            "print(classification_report(sentiments_test, sentiments_pred))\n",
            "\n",
            "cm = confusion_matrix(sentiments_test, sentiments_pred)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#SVM Classifier**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = LinearSVC(dual=True, max_iter=10000, class_weight=svm_rf_weight)\n",
            "\n",
            "clf.fit(reviews_train, sentiments_train)\n",
            "\n",
            "dump(clf, 'svm_partition.joblib')\n",
            "svm_loaded = load('svm.joblib')\n",
            "\n",
            "sentiments_pred = clf.predict(reviews_test)\n",
            "train_accuracy = accuracy_score(sentiments_train, clf.predict(reviews_train))\n",
            "test_accuracy = accuracy_score(sentiments_test, clf.predict(reviews_test))\n",
            "print(\"Train accuracy: \", train_accuracy)\n",
            "print(\"Test accuracy: \", test_accuracy)\n",
            "print(classification_report(sentiments_test, sentiments_pred))\n",
            "\n",
            "cm = confusion_matrix(sentiments_test, sentiments_pred)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Multinomial Naive Bayes Classifier**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf = MultinomialNB(class_prior=bayes_weight)\n",
            "clf.fit(reviews_train, sentiments_train)\n",
            "dump(clf, \"bayes_partition.joblib\")\n",
            "\n",
            "nb_loaded = load('bayes.joblib')\n",
            "sentiments_pred = clf.predict(reviews_test)\n",
            "\n",
            "print(classification_report(sentiments_test, sentiments_pred))\n",
            "\n",
            "train_accuracy = accuracy_score(sentiments_train, clf.predict(reviews_train))\n",
            "test_accuracy = accuracy_score(sentiments_test, clf.predict(reviews_test))\n",
            "print(\"Train accuracy: \", train_accuracy)\n",
            "print(\"Test accuracy: \", test_accuracy)\n",
            "\n",
            "cm = confusion_matrix(sentiments_test, sentiments_pred)\n",
            "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
            "plt.title(\"Confusion Matrix\")\n",
            "plt.xlabel(\"Predicted\")\n",
            "plt.ylabel(\"True\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#WordCloud**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "positive_reviews = df[df['sentiment'] == 2]['reviewText'].str.cat(sep=' ')\n",
            "wordcloud = WordCloud(background_color='white', max_words=200).generate(positive_reviews)\n",
            "plt.figure(figsize=(10, 10))\n",
            "plt.imshow(wordcloud, interpolation='bilinear')\n",
            "plt.axis('off')\n",
            "plt.title('Word Cloud for Positive Reviews')\n",
            "plt.show()\n",
            "\n",
            "negative_reviews = df[df['sentiment'] == 0]['reviewText'].str.cat(sep=' ')\n",
            "wordcloud_negative = WordCloud(background_color='white', max_words=200, contour_color='red').generate(negative_reviews)\n",
            "plt.figure(figsize=(10, 10))\n",
            "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
            "plt.axis('off')\n",
            "plt.title('Negative Sentiment Word Cloud')\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**#Metrics Graph**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "svm_loaded = load('svm_partition.joblib')\n",
            "nb_loaded = load('bayes_partition.joblib')\n",
            "precision_rf, recall_rf, fscore_rf, _ = score(sentiments_test, rf_loaded.predict(reviews_test), average='macro')\n",
            "\n",
            "precision_svm, recall_svm, fscore_svm, _ = score(sentiments_test, svm_loaded.predict(reviews_test), average='macro')\n",
            "\n",
            "precision_nb, recall_nb, fscore_nb, _ = score(sentiments_test, nb_loaded.predict(reviews_test), average='macro')\n",
            "\n",
            "models = ['Random Forest', 'SVM', 'Naive Bayes']\n",
            "precision_scores = [precision_rf, precision_svm, precision_nb]\n",
            "recall_scores = [recall_rf, recall_svm, recall_nb]\n",
            "fscore_scores = [fscore_rf, fscore_svm, fscore_nb]\n",
            "\n",
            "x = np.arange(len(models))  \n",
            "width = 0.25  \n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "rects1 = ax.bar(x - width, precision_scores, width, label='Precision')\n",
            "rects2 = ax.bar(x, recall_scores, width, label='Recall')\n",
            "rects3 = ax.bar(x + width, fscore_scores, width, label='F1-Score')\n",
            "\n",
            "ax.set_ylabel('Scores')\n",
            "ax.set_title('Scores by model and metric')\n",
            "ax.set_xticks(x)\n",
            "ax.set_xticklabels(models)\n",
            "ax.legend()\n",
            "\n",
            "fig.tight_layout()\n",
            "plt.show()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.2"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
